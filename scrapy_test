import scrapy
from fake_useragent import UserAgent


class FoshanspiderSpider(scrapy.Spider):
   name = "scrapy_test"
   allowed_domains = ["www.fsgykg.com"]
   start_urls = ["https://www.fsgykg.com/index/Article/info.html?cate=70&id=3176"]
    
   def parse(self, response):
       # Use the UserAgent object to set the user agent string
       ua = UserAgent()
       response.headers['User-Agent'] = ua.random
       print(response.headers['User-Agent'])

       # Extract the data
       company = response.xpath("//h2[contains(text(), '佛山市国际贸易有限公司')]/text()").get().replace('\n', ' ').strip()
       view = response.css("div.news-details:nth-child(4) div.con.w1440 div.center div.top div.txt.clearfix:nth-child(2) > h4:nth-child(2)::text").get()
       body = response.css('div.news-details:nth-child(4) div.con.w1440 div.center div.text p:nth-child(1) span:nth-child(1) > span:nth-child(1)::text').get().replace('\n', ' ').strip()
       image_urls = response.xpath('/html[1]/body[1]/div[3]/div[1]/div[1]/div[2]/p[2]/img[1]/@src').getall()
       date = response.xpath('//h3[contains(text(),"2023.02.01")]/text()').get().replace('\n', ' ').strip()

       # Join each image URL with the response URL
       #image_urls = [response.urljoin(url) for url in image_urls]

       yield {
          'company': company,
          'view': view,
          'body': body,
           'date': date,
           'image_urls': image_urls # use urljoin to get absolute URL
       }
 
